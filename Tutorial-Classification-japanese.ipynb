{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# このノートブックについて\n",
        "* OpenPromptを使って、Livedoorニュースコーパスの分類問題を解きます\n",
        "* [公式のチュートリアルスクリプト](https://github.com/thunlp/OpenPrompt/blob/fe7f4cbb719e796311973c882883773c8306c4b2/tutorial/1.2_soft_verbalizers.py)をベースに、Colab上で動かせるように改変しています\n",
        "* 学習に時間がかかるため、Colab利用時はランタイムタイプを変更してGPUを有効化することを推奨します"
      ],
      "metadata": {
        "id": "eLOIMntG3d73"
      },
      "id": "eLOIMntG3d73"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive設定\n",
        "Colab環境でファイルを永続化するため、Google Driveをマウントして保存用ディレクトリを作成\n",
        "\n",
        "* マウント時に認証を要求されるので許可が必要"
      ],
      "metadata": {
        "id": "HsvX85BZ2mix"
      },
      "id": "HsvX85BZ2mix"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./gdrive')"
      ],
      "metadata": {
        "id": "UeqWGRISE6ar"
      },
      "id": "UeqWGRISE6ar",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p gdrive/MyDrive/openprompt/result gdrive/MyDrive/openprompt/models"
      ],
      "metadata": {
        "id": "YkUkP48vYLpx"
      },
      "id": "YkUkP48vYLpx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 必要なライブラリをインストール\n",
        "* 注意\n",
        "    * ColabのPythonバージョンが3.7系のため、OpenPromptの0.1.1までしかインストールできません\n",
        "    * 自前のJupyter環境で動かす場合は、最新バージョンを利用してもよいかもです（そのままコードが動くかは保証しませんが）"
      ],
      "metadata": {
        "id": "mV6aGnqU3B1u"
      },
      "id": "mV6aGnqU3B1u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5eefca9-3d17-409b-946c-709bc0e997e4",
      "metadata": {
        "id": "d5eefca9-3d17-409b-946c-709bc0e997e4"
      },
      "outputs": [],
      "source": [
        "!pip install openprompt==0.1.1 \\\n",
        "ja_sentence_segmenter \\\n",
        "'torch>=1.9.0' \\\n",
        "'transformers>=4.10.0' \\\n",
        "sentencepiece==0.1.96 \\\n",
        "'scikit-learn>=0.24.2' \\\n",
        "'tqdm>=4.62.2' \\\n",
        "tensorboardX \\\n",
        "nltk \\\n",
        "yacs \\\n",
        "dill \\\n",
        "datasets \\\n",
        "rouge==1.0.0 \\\n",
        "scipy==1.4.1 \\\n",
        "fugashi \\\n",
        "ipadic \\\n",
        "unidic-lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "242bc861-a22e-4cd5-9729-241e729dc8b4",
      "metadata": {
        "id": "242bc861-a22e-4cd5-9729-241e729dc8b4"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data\n",
        "!test -e ldcc-20140209.tar.gz || wget -O ldcc-20140209.tar.gz https://www.rondhuit.com/download/ldcc-20140209.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 実行ごとにファイル名がかぶらないようにランダム文字列を取得\n",
        "import random\n",
        "this_run_unicode = str(random.randint(0, 1e10))"
      ],
      "metadata": {
        "id": "oWAYQf26YBO7"
      },
      "id": "oWAYQf26YBO7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 変数宣言\n",
        "[公式チュートリアルのスクリプト](https://github.com/thunlp/OpenPrompt/blob/main/tutorial/1.4_soft_template.py)で使われているコマンドライン引数等をここで設定できるようにしてあります。\n",
        "\n",
        "各変数の説明は、上記スクリプトをご参照ください。"
      ],
      "metadata": {
        "id": "UgHzgNzx1oMm"
      },
      "id": "UgHzgNzx1oMm"
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "\n",
        "# FewShotラーニング時のラベルごとのデータ数\n",
        "num_examples_per_label = 16 # Noneで全量使用\n",
        "\n",
        "# プロンプトラーニング用パラメータ\n",
        "# model = \"t5\"\n",
        "# model_name_or_path = \"sonoisa/t5-base-japanese\"\n",
        "model = \"bert-ja\"\n",
        "model_name_or_path = \"cl-tohoku/bert-large-japanese\"\n",
        "shot = 1\n",
        "tune_plm = True\n",
        "plm_eval_mode = False\n",
        "max_steps = 1000\n",
        "eval_every_steps = 100\n",
        "prompt_lr = 0.3\n",
        "warmup_step_prompt = 100\n",
        "optimizer = \"Adafactor\"\n",
        "multi_token_handler = \"max\" # \"first\" or \"mean\" or \"max\"\n",
        "truncate_method=\"tail\" # \"head\" or \"tail\" or \"balanced\"\n",
        "\n",
        "\n",
        "# 環境スペックに応じて適宜変更してください\n",
        "# batchsize_t = 4\n",
        "# batchsize_e = 4\n",
        "batchsize_t = 2\n",
        "batchsize_e = 2\n",
        "max_seq_l = 512\n",
        "gradient_accumulation_steps = 8\n",
        "model_parallelize = False\n",
        "use_cuda = True\n",
        "\n",
        "# 結果の保存先・ファイル名\n",
        "project_root = \"/content/gdrive/MyDrive/openprompt/\"\n",
        "import datetime\n",
        "dt_now = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
        "result_file = f\"results/results-{model}-{shot}-{dt_now}.txt\""
      ],
      "metadata": {
        "id": "2sIo8QLhhNfb"
      },
      "id": "2sIo8QLhhNfb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openprompt.utils.reproduciblity import set_seed\n",
        "\n",
        "set_seed(seed)"
      ],
      "metadata": {
        "id": "HrHQXxEuPqzm"
      },
      "id": "HrHQXxEuPqzm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Livedoorニュースコーパスの準備\n",
        "* [sonoisa/t5-japanese](https://github.com/sonoisa/t5-japanese)のノートブックを参考に、OpenPrompt用の形式に変換しています"
      ],
      "metadata": {
        "id": "cFyzGWpK0Vtl"
      },
      "id": "cFyzGWpK0Vtl"
    },
    {
      "cell_type": "code",
      "source": [
        "target_genres = [\"dokujo-tsushin\",\n",
        "                 \"it-life-hack\",\n",
        "                 \"kaden-channel\",\n",
        "                 \"livedoor-homme\",\n",
        "                 \"movie-enter\",\n",
        "                 \"peachy\",\n",
        "                 \"smax\",\n",
        "                 \"sports-watch\",\n",
        "                 \"topic-news\"]"
      ],
      "metadata": {
        "id": "wMEUmFbcEdxy"
      },
      "id": "wMEUmFbcEdxy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1e544e3-5742-44e6-8716-317d978068f3",
      "metadata": {
        "id": "e1e544e3-5742-44e6-8716-317d978068f3"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import re\n",
        "\n",
        "from ja_sentence_segmenter.normalize.neologd_normalizer import normalize\n",
        "\n",
        "def remove_brackets(text):\n",
        "    text = re.sub(r\"(^【[^】]*】)|(【[^】]*】$)\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def normalize_text(text):\n",
        "    assert \"\\n\" not in text and \"\\r\" not in text\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "    text = text.strip()\n",
        "    text = list(normalize(text))[0]\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def read_title_body(file):\n",
        "    next(file)\n",
        "    next(file)\n",
        "    title = next(file).decode(\"utf-8\").strip()\n",
        "    title = normalize_text(remove_brackets(title))\n",
        "    body = normalize_text(\" \".join([line.decode(\"utf-8\").strip() for line in file.readlines()]))\n",
        "    return title, body\n",
        "\n",
        "genre_files_list = [[] for genre in target_genres]\n",
        "\n",
        "all_data = []\n",
        "\n",
        "with tarfile.open(\"ldcc-20140209.tar.gz\") as archive_file:\n",
        "    for archive_item in archive_file:\n",
        "        for i, genre in enumerate(target_genres):\n",
        "            if genre in archive_item.name and archive_item.name.endswith(\".txt\"):\n",
        "                genre_files_list[i].append(archive_item.name)\n",
        "\n",
        "    for i, genre_files in enumerate(genre_files_list):\n",
        "        for name in genre_files:\n",
        "            file = archive_file.extractfile(name)\n",
        "            title, body = read_title_body(file)\n",
        "            title = normalize_text(title)\n",
        "            body = normalize_text(body)\n",
        "\n",
        "            if len(title) > 0 and len(body) > 0:\n",
        "                all_data.append({\n",
        "                    \"title\": title,\n",
        "                    \"body\": body,\n",
        "                    \"genre_id\": i\n",
        "                    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73397c2c-7821-4e7f-86b9-e5d1946d8900",
      "metadata": {
        "id": "73397c2c-7821-4e7f-86b9-e5d1946d8900"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "random.shuffle(all_data)\n",
        "\n",
        "def to_line(data):\n",
        "    title = data[\"title\"]\n",
        "    body = data[\"body\"]\n",
        "    genre_id = data[\"genre_id\"]\n",
        "\n",
        "    assert len(title) > 0 and len(body) > 0\n",
        "    return f\"{title}\\t{body}\\t{genre_id}\\n\"\n",
        "\n",
        "data_size = len(all_data)\n",
        "train_ratio, dev_ratio, test_ratio = 0.7, 0.15, 0.15\n",
        "\n",
        "with open(f\"data/train.tsv\", \"w\", encoding=\"utf-8\") as f_train, \\\n",
        "    open(f\"data/dev.tsv\", \"w\", encoding=\"utf-8\") as f_dev, \\\n",
        "    open(f\"data/test.tsv\", \"w\", encoding=\"utf-8\") as f_test:\n",
        "    \n",
        "    for i, data in tqdm(enumerate(all_data)):\n",
        "        line = to_line(data)\n",
        "        if i < train_ratio * data_size:\n",
        "            f_train.write(line)\n",
        "        elif i < (train_ratio + dev_ratio) * data_size:\n",
        "            f_dev.write(line)\n",
        "        else:\n",
        "            f_test.write(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c96cfb1f-d87b-4fc0-ab64-7a72ea803b07",
      "metadata": {
        "id": "c96cfb1f-d87b-4fc0-ab64-7a72ea803b07"
      },
      "outputs": [],
      "source": [
        "import openprompt.plms as plms\n",
        "from openprompt.plms.mlm import MLMTokenizerWrapper\n",
        "from transformers import BertConfig, BertForMaskedLM, BertJapaneseTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plms._MODEL_CLASSES['bert-ja'] = plms.ModelClass(**{\n",
        "    'config': BertConfig,\n",
        "    'tokenizer': BertJapaneseTokenizer,\n",
        "    'model':BertForMaskedLM,\n",
        "    'wrapper': MLMTokenizerWrapper,\n",
        "})"
      ],
      "metadata": {
        "id": "cu3tz-4NC-Di"
      },
      "id": "cu3tz-4NC-Di",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed9ca7e0-7f76-4fdb-8253-7b7d09522323",
      "metadata": {
        "id": "ed9ca7e0-7f76-4fdb-8253-7b7d09522323"
      },
      "outputs": [],
      "source": [
        "plms._MODEL_CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c375f1f0-82df-4e52-9be4-a320a9326604",
      "metadata": {
        "id": "c375f1f0-82df-4e52-9be4-a320a9326604"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openprompt.data_utils.data_processor import DataProcessor\n",
        "from openprompt.data_utils.utils import InputExample\n",
        "\n",
        "class LivedoorNewsProcessor(DataProcessor):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.labels = target_genres\n",
        "\n",
        "    def get_examples(self, data_dir, split):\n",
        "        path = os.path.join(data_dir, \"{}.tsv\".format(split))\n",
        "        examples = []\n",
        "        with open(path, \"r\", encoding='utf8') as f:\n",
        "            for idx, line in enumerate(f):\n",
        "                line = line.strip().split(\"\\t\")\n",
        "                \n",
        "                text_a = line[0]\n",
        "                text_b = line[1]\n",
        "                label = line[2]\n",
        "                \n",
        "                example = InputExample(guid=str(idx), text_a=text_a, text_b=text_b, label=int(label))\n",
        "                examples.append(example)\n",
        "        return examples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = {}\n",
        "dataset['train'] = LivedoorNewsProcessor().get_train_examples(\"./data\")\n",
        "dataset['dev'] = LivedoorNewsProcessor().get_dev_examples(\"./data\")\n",
        "dataset['test'] = LivedoorNewsProcessor().get_test_examples(\"./data\")\n",
        "class_labels = LivedoorNewsProcessor().get_labels()"
      ],
      "metadata": {
        "id": "wOhLkHeLQRIp"
      },
      "id": "wOhLkHeLQRIp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モデル学習\n"
      ],
      "metadata": {
        "id": "5oTAxuto0sD4"
      },
      "id": "5oTAxuto0sD4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4820137c-434b-4f49-a235-3a97742c0a0b",
      "metadata": {
        "id": "4820137c-434b-4f49-a235-3a97742c0a0b"
      },
      "outputs": [],
      "source": [
        "from openprompt.plms import load_plm\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(model, model_name_or_path)\n",
        "# plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"sonoisa/t5-base-japanese\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openprompt.prompts import ManualTemplate\n",
        "mytemplate = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"} この記事のジャンルは{\"mask\"}。')"
      ],
      "metadata": {
        "id": "jCJ57gpJsOqi"
      },
      "id": "jCJ57gpJsOqi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f859015-ce7c-484a-827b-32fb8c436159",
      "metadata": {
        "id": "4f859015-ce7c-484a-827b-32fb8c436159"
      },
      "outputs": [],
      "source": [
        "from openprompt.data_utils.data_sampler import FewShotSampler\n",
        "\n",
        "if num_examples_per_label is not None:\n",
        "    sampler  = FewShotSampler(num_examples_per_label=num_examples_per_label)\n",
        "    dataset['train'] = sampler(dataset['train'], seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a07648a4-66ba-4fe0-b066-7ed4f8577f13",
      "metadata": {
        "id": "a07648a4-66ba-4fe0-b066-7ed4f8577f13"
      },
      "outputs": [],
      "source": [
        "from openprompt import PromptDataLoader\n",
        "\n",
        "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer, \n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3, \n",
        "    batch_size=batchsize_t,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=truncate_method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "747e7956-0ec2-4cf2-88b7-49af38fc0aad",
      "metadata": {
        "id": "747e7956-0ec2-4cf2-88b7-49af38fc0aad"
      },
      "outputs": [],
      "source": [
        "validation_dataloader = PromptDataLoader(dataset=dataset[\"dev\"], template=mytemplate, tokenizer=tokenizer, \n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3, \n",
        "    batch_size=batchsize_e,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=truncate_method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ba18066-a262-4cd0-aeef-1d0fbbe688bb",
      "metadata": {
        "id": "1ba18066-a262-4cd0-aeef-1d0fbbe688bb"
      },
      "outputs": [],
      "source": [
        "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer, \n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3, \n",
        "    batch_size=batchsize_e,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=truncate_method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c55b67d-0cea-42d8-a63b-4300985e2e1e",
      "metadata": {
        "id": "8c55b67d-0cea-42d8-a63b-4300985e2e1e"
      },
      "outputs": [],
      "source": [
        "from openprompt.prompts import SoftVerbalizer\n",
        "import torch\n",
        "\n",
        "# TODO：label_wordsを設定するとうまく動かない\n",
        "# https://github.com/thunlp/OpenPrompt/issues/78\n",
        "# label_words = [\n",
        "#                 \"独女\",\n",
        "#                 \"ライフハック\",\n",
        "#                 \"家電\",\n",
        "#                 \"メンズ\",\n",
        "#                 \"映画\",\n",
        "#                 \"恋愛\",\n",
        "#                 \"ガジェット\",\n",
        "#                 \"スポーツ\",\n",
        "#                 \"ニュース\"\n",
        "# ]\n",
        "# myverbalizer = SoftVerbalizer(tokenizer=tokenizer, plm=plm, label_words=label_words, classes=target_genres, num_classes=9,multi_token_handler=\"max\")\n",
        "\n",
        "myverbalizer = SoftVerbalizer(tokenizer=tokenizer, plm=plm, classes=target_genres, num_classes=9, multi_token_handler=multi_token_handler)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def evaluate(prompt_model, dataloader, desc):\n",
        "    prompt_model.eval()\n",
        "    allpreds = []\n",
        "    alllabels = []\n",
        "   \n",
        "    for step, inputs in enumerate(dataloader):\n",
        "        if use_cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        logits = prompt_model(inputs)\n",
        "        labels = inputs['label']\n",
        "        alllabels.extend(labels.cpu().tolist())\n",
        "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "    \n",
        "    print(confusion_matrix(alllabels, allpreds))\n",
        "    print(classification_report(alllabels, allpreds))\n",
        "    return acc"
      ],
      "metadata": {
        "id": "bTGduaxDqomk"
      },
      "id": "bTGduaxDqomk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26db6256-96d2-45a4-b067-2fddba8038a2",
      "metadata": {
        "id": "26db6256-96d2-45a4-b067-2fddba8038a2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "434d891a-726b-49de-ba9f-f5a5646a0682",
      "metadata": {
        "id": "434d891a-726b-49de-ba9f-f5a5646a0682"
      },
      "outputs": [],
      "source": [
        "from openprompt import PromptForClassification\n",
        "myPromptModel = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=(not tune_plm), plm_eval_mode=plm_eval_mode)\n",
        "if use_cuda:\n",
        "    myPromptModel=  myPromptModel.cuda()\n",
        "if model_parallelize:\n",
        "    myPromptModel.parallelize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "880d2ea4-3ae1-4c33-ac46-3947bad6239c",
      "metadata": {
        "id": "880d2ea4-3ae1-4c33-ac46-3947bad6239c"
      },
      "outputs": [],
      "source": [
        "from transformers import  AdamW, get_linear_schedule_with_warmup,get_constant_schedule_with_warmup  # use AdamW is a standard practice for transformer \n",
        "from transformers.optimization import Adafactor, AdafactorSchedule  # use Adafactor is the default setting for T5\n",
        "\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "tot_step = max_steps\n",
        "\n",
        "if tune_plm: # normally we freeze the model when using soft_template. However, we keep the option to tune plm\n",
        "    no_decay = ['bias', 'LayerNorm.weight'] # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters1 = [\n",
        "        {'params': [p for n, p in myPromptModel.plm.named_parameters() if (not any(nd in n for nd in no_decay))], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in myPromptModel.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
        "    scheduler1 = get_linear_schedule_with_warmup(\n",
        "        optimizer1, \n",
        "        num_warmup_steps=warmup_step_prompt, num_training_steps=tot_step)\n",
        "else:\n",
        "    optimizer1 = None\n",
        "    scheduler1 = None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_grouped_parameters2 = [{'params': [p for name, p in myPromptModel.template.named_parameters() if 'raw_embedding' not in name]}] # note that you have to remove the raw_embedding manually from the optimization\n",
        "if optimizer.lower() == \"adafactor\":\n",
        "    optimizer2 = Adafactor(optimizer_grouped_parameters2,  \n",
        "                            lr=prompt_lr,\n",
        "                            relative_step=False,\n",
        "                            scale_parameter=False,\n",
        "                            warmup_init=False)  # when lr is 0.3, it is the same as the configuration of https://arxiv.org/abs/2104.08691\n",
        "    scheduler2 = get_constant_schedule_with_warmup(optimizer2, num_warmup_steps=warmup_step_prompt) # when num_warmup_steps is 0, it is the same as the configuration of https://arxiv.org/abs/2104.08691\n",
        "elif optimizer.lower() == \"adamw\":\n",
        "    optimizer2 = AdamW(optimizer_grouped_parameters2, lr=prompt_lr) # usually lr = 0.5\n",
        "    scheduler2 = get_linear_schedule_with_warmup(\n",
        "                    optimizer2, \n",
        "                    num_warmup_steps=warmup_step_prompt, num_training_steps=tot_step) # usually num_warmup_steps is 500"
      ],
      "metadata": {
        "id": "uhyBD10WrRrb"
      },
      "id": "uhyBD10WrRrb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_write = \"=\"*20+\"\\n\"\n",
        "content_write += f\"model:{model}\\t\"\n",
        "content_write += f\"model_name_or_path:{model_name_or_path}\\t\"\n",
        "content_write += f\"seed:{seed}\\t\"\n",
        "content_write += f\"shot:{shot}\\t\"\n",
        "content_write += f\"num_examples_per_label:{num_examples_per_label}\\t\"\n",
        "content_write += f\"plm_eval_mode:{plm_eval_mode}\\t\"\n",
        "content_write += f\"eval_every_steps:{eval_every_steps}\\t\"\n",
        "content_write += f\"warmup_step_prompt:{warmup_step_prompt}\\t\"\n",
        "content_write += f\"prompt_lr:{prompt_lr}\\t\"\n",
        "content_write += f\"optimizer:{optimizer}\\t\"\n",
        "content_write += f\"multi_token_handler:{multi_token_handler}\\t\"\n",
        "content_write += \"\\n\"\n",
        "content_write += f\"batchsize_t:{batchsize_t}\\t\"\n",
        "content_write += f\"batchsize_e:{batchsize_e}\\t\"\n",
        "content_write += f\"max_seq_l:{max_seq_l}\\t\"\n",
        "content_write += f\"gradient_accumulation_steps:{gradient_accumulation_steps}\\t\"\n",
        "content_write += f\"model_parallelize:{model_parallelize}\\t\"\n",
        "content_write += f\"use_cuda:{use_cuda}\\t\"\n",
        "\n",
        "print(content_write)"
      ],
      "metadata": {
        "id": "xilvwT1ivKhI"
      },
      "id": "xilvwT1ivKhI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1af989d-32fa-4900-bf89-a0ecfc517acc",
      "metadata": {
        "id": "d1af989d-32fa-4900-bf89-a0ecfc517acc"
      },
      "outputs": [],
      "source": [
        "tot_loss = 0 \n",
        "log_loss = 0\n",
        "best_val_acc = 0\n",
        "glb_step = 0\n",
        "actual_step = 0\n",
        "leave_training = False\n",
        "\n",
        "acc_traces = []\n",
        "tot_train_time = 0\n",
        "pbar_update_freq = 10\n",
        "myPromptModel.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "pbar = tqdm(total=tot_step, desc=\"Train\")\n",
        "for epoch in range(1000000):\n",
        "    # print(f\"Begin epoch {epoch}\")\n",
        "    pbar.set_description(f\"Train[Epoch {epoch}]\")\n",
        "    for step, inputs in enumerate(train_dataloader):\n",
        "        if use_cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        tot_train_time -= time.time()\n",
        "        logits = myPromptModel(inputs)\n",
        "        labels = inputs['label']\n",
        "        loss = loss_func(logits, labels)\n",
        "        loss.backward()\n",
        "        tot_loss += loss.item()\n",
        "        actual_step += 1\n",
        "\n",
        "        if actual_step % gradient_accumulation_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(myPromptModel.parameters(), 1.0)\n",
        "            glb_step += 1\n",
        "            if glb_step % pbar_update_freq == 0:\n",
        "                aveloss = (tot_loss - log_loss)/pbar_update_freq\n",
        "                pbar.update(10)\n",
        "                pbar.set_postfix({'loss': aveloss})\n",
        "                log_loss = tot_loss\n",
        "\n",
        "        \n",
        "        if optimizer1 is not None:\n",
        "            optimizer1.step()\n",
        "            optimizer1.zero_grad()\n",
        "        if scheduler1 is not None:\n",
        "            scheduler1.step()\n",
        "        if optimizer2 is not None:\n",
        "            optimizer2.step()\n",
        "            optimizer2.zero_grad()\n",
        "        if scheduler2 is not None:\n",
        "            scheduler2.step()\n",
        "\n",
        "        tot_train_time += time.time()\n",
        "\n",
        "        if actual_step % gradient_accumulation_steps == 0 and glb_step >0 and glb_step % eval_every_steps == 0:\n",
        "            val_acc = evaluate(myPromptModel, validation_dataloader, desc=\"Valid\")\n",
        "            if val_acc >= best_val_acc:\n",
        "                torch.save(myPromptModel.state_dict(),f\"{project_root}models/{this_run_unicode}.ckpt\")\n",
        "                best_val_acc = val_acc\n",
        "            \n",
        "            acc_traces.append(val_acc)\n",
        "            print(\"Glb_step {}, val_acc {}, average time {}\".format(glb_step, val_acc, tot_train_time/actual_step ), flush=True)\n",
        "            myPromptModel.train()\n",
        "\n",
        "        if glb_step > max_steps:\n",
        "            leave_training = True\n",
        "            break\n",
        "    \n",
        "    if leave_training:\n",
        "        break  "
      ],
      "metadata": {
        "id": "640u576Mtv56"
      },
      "id": "640u576Mtv56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myPromptModel.load_state_dict(torch.load(f\"{project_root}/models/{this_run_unicode}.ckpt\"))\n",
        "test_acc = evaluate(myPromptModel, test_dataloader, desc=\"Test\")"
      ],
      "metadata": {
        "id": "GjrUc_tG056-"
      },
      "id": "GjrUc_tG056-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ff5342-eea7-475c-8058-c5d15f226b44",
      "metadata": {
        "id": "86ff5342-eea7-475c-8058-c5d15f226b44"
      },
      "outputs": [],
      "source": [
        "# a simple measure for the convergence speed.\n",
        "thres99 = 0.99*best_val_acc\n",
        "thres98 = 0.98*best_val_acc\n",
        "thres100 = best_val_acc\n",
        "step100=step98=step99=max_steps\n",
        "for val_time, acc in enumerate(acc_traces):\n",
        "    if acc>=thres98:\n",
        "        step98 = min(val_time*eval_every_steps, step98)\n",
        "        if acc>=thres99:\n",
        "            step99 = min(val_time*eval_every_steps, step99)\n",
        "            if acc>=thres100:\n",
        "                step100 = min(val_time*eval_every_steps, step100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content_write += f\"BestValAcc:{best_val_acc}\\tEndValAcc:{acc_traces[-1]}\\tcritical_steps:{[step98,step99,step100]}\\n\"\n",
        "content_write += f\"testAcc:{test_acc}\\n\"\n",
        "content_write += \"\\n\"\n",
        "\n",
        "print(content_write)\n",
        "\n",
        "with open(f\"{project_root}{result_file}\", \"a\") as fout:\n",
        "    fout.write(content_write)"
      ],
      "metadata": {
        "id": "aKFBGCLwxz2q"
      },
      "id": "aKFBGCLwxz2q",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "text_representation": {
        "extension": ".py",
        "format_name": "light",
        "format_version": "1.5",
        "jupytext_version": "1.12.0"
      }
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "Tutorial-Classification-japanese.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yTWFsFt1hh2Z",
        "HmzF1FqElfL6"
      ],
      "private_outputs": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}