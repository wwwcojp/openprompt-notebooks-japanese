{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction-japanese.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# このノートブックについて\n",
        "* OpenPromptを使って、[公式のイントロダクション](https://github.com/thunlp/OpenPrompt#use-openprompt)を日本語モデル・データで動かしてみます\n",
        "* 各コードの説明も上記イントロダクションからの引用です"
      ],
      "metadata": {
        "id": "jOouWbs244nw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 必要なライブラリをインストール\n",
        "* 注意\n",
        "    * ColabのPythonバージョンが3.7系のため、OpenPromptの0.1.1までしかインストールできません\n",
        "    * 自前のJupyter環境で動かす場合は、最新バージョンを利用してもよいかもです（そのままコードが動くかは保証しませんが）"
      ],
      "metadata": {
        "id": "6qQs0h4q5fnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openprompt==0.1.1 \\\n",
        "'torch>=1.9.0' \\\n",
        "'transformers>=4.10.0' \\\n",
        "sentencepiece==0.1.96 \\\n",
        "'scikit-learn>=0.24.2' \\\n",
        "'tqdm>=4.62.2' \\\n",
        "tensorboardX \\\n",
        "nltk \\\n",
        "yacs \\\n",
        "dill \\\n",
        "datasets \\\n",
        "rouge==1.0.0 \\\n",
        "scipy==1.4.1 \\\n",
        "fugashi \\\n",
        "ipadic \\\n",
        "unidic-lite"
      ],
      "metadata": {
        "id": "Bgm_jBuEbJb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 日本語BERTモデルの利用設定\n",
        "現状、OpenPromptでは言語モデルの種類（BERT,T5,GPT,...）ごとに利用するトークナイザがハードコーディングされています。\n",
        "\n",
        "そのため、日本語BERTモデル用の設定をやや無理矢理入れ込みます。"
      ],
      "metadata": {
        "id": "h40NMlRG5qWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openprompt.plms as plms\n",
        "from openprompt.plms.mlm import MLMTokenizerWrapper\n",
        "from transformers import BertConfig, BertForMaskedLM, BertJapaneseTokenizer"
      ],
      "metadata": {
        "id": "rHra-taOa7_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plms._MODEL_CLASSES['bert-ja'] = plms.ModelClass(**{\n",
        "    'config': BertConfig,\n",
        "    'tokenizer': BertJapaneseTokenizer,\n",
        "    'model':BertForMaskedLM,\n",
        "    'wrapper': MLMTokenizerWrapper,\n",
        "})"
      ],
      "metadata": {
        "id": "OZWrokM5cF8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plms._MODEL_CLASSES"
      ],
      "metadata": {
        "id": "gsxMMn8pdQx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Define a task\n",
        "The first step is to determine the current NLP task, think about what’s your data looks like and what do you want from the data! That is, the essence of this step is to determine the classses and the InputExample of the task. For simplicity, we use Sentiment Analysis as an example. tutorial_task."
      ],
      "metadata": {
        "id": "lqqf6NKRKVgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openprompt.data_utils import InputExample\n",
        "classes = [ # There are two classes in Sentiment Analysis, one for negative and one for positive\n",
        "    \"negative\",\n",
        "    \"positive\"\n",
        "]\n",
        "dataset = [ # For simplicity, there's only two examples\n",
        "    # text_a is the input text of the data, some other datasets may have multiple input sentences in one example.\n",
        "    InputExample(\n",
        "        guid = 0,\n",
        "        text_a = \"私はそのホラー映画が面白くて三度の飯より好きです。\", #positive\n",
        "    ),\n",
        "    InputExample(\n",
        "        guid = 1,\n",
        "        text_a = \"このサメ映画はとてもつまらない駄作だと思います。\", #negative\n",
        "    ),\n",
        "    InputExample(\n",
        "        guid = 2,\n",
        "        text_a = \"その映画は私の中でベスト3に入る良い作品だった。\", #positive\n",
        "    ),\n",
        "    InputExample(\n",
        "        guid = 3,\n",
        "        text_a = \"この感動巨編なラブロマンスはとても退屈で眠ってしまった。\", #negative\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "YnhS5PiJJuPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Define a Pre-trained Language Models (PLMs) as backbone.\n",
        "Choose a PLM to support your task. Different models have different attributes, we encourge you to use OpenPrompt to explore the potential of various PLMs. OpenPrompt is compatible with models on huggingface.\n"
      ],
      "metadata": {
        "id": "DymjAiXFKYw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "from openprompt.plms import load_plm\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert-ja\", \"cl-tohoku/bert-large-japanese\")"
      ],
      "metadata": {
        "id": "rFp3ya01J3nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(\"私はこのサメ映画はとてもつまらない駄作だと思います。\")"
      ],
      "metadata": {
        "id": "bM7D10XJg49q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Define a Template.\n",
        "A Template is a modifier of the original input text, which is also one of the most important modules in prompt-learning.  We have defined text_a in Step 1."
      ],
      "metadata": {
        "id": "zsI0hUZyKe4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openprompt.prompts import ManualTemplate\n",
        "# template_text = '{\"placeholder\":\"text_a\"}: この意見は{\"mask\"}な評価です。'\n",
        "# template_text = '{\"placeholder\":\"text_a\"}この意見は{\"mask\"}な評価です。'\n",
        "template_text = '{\"placeholder\":\"text_a\"}: この意見は{\"mask\"}な評価です。'\n",
        "\n",
        "promptTemplate = ManualTemplate(\n",
        "    text = template_text,\n",
        "    tokenizer = tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "HG7p1ETaKg4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Define a Verbalizer\n",
        "A Verbalizer is another important (but not neccessary) in prompt-learning,which projects the original labels (we have defined them as classes, remember?) to a set of label words. Here is an example that we project the negative class to the word bad, and project the positive class to the words good, wonderful, great."
      ],
      "metadata": {
        "id": "hthNbpJCKh9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openprompt.prompts import ManualVerbalizer\n",
        "promptVerbalizer = ManualVerbalizer(\n",
        "    classes = classes,\n",
        "    label_words = {\n",
        "        \"negative\": [\"ネガティブ\", \"否定的\", \"つまらない\", \"駄作\"],\n",
        "        \"positive\": [\"ポジティブ\", \"肯定的\", \"面白い\", \"名作\"],\n",
        "    },\n",
        "    tokenizer = tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "QALc5AqJKl6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Combine them into a PromptModel\n",
        "Given the task, now we have a PLM, a Template and a Verbalizer, we combine them into a PromptModel. Note that although the example naively combine the three modules, you can actually define some complicated interactions among them."
      ],
      "metadata": {
        "id": "WD8leL4PKpNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openprompt import PromptForClassification\n",
        "promptModel = PromptForClassification(\n",
        "    template = promptTemplate,\n",
        "    plm = plm,\n",
        "    verbalizer = promptVerbalizer\n",
        ")"
      ],
      "metadata": {
        "id": "U-qTGso-Kqez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Define a DataLoader\n",
        "A PromptDataLoader is basically a prompt version of pytorch Dataloader, which also includes a Tokenizer, a Template and a TokenizerWrapper."
      ],
      "metadata": {
        "id": "4E7vuggeKuCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openprompt import PromptDataLoader\n",
        "data_loader = PromptDataLoader(\n",
        "    dataset = dataset,\n",
        "    tokenizer = tokenizer, \n",
        "    template = promptTemplate, \n",
        "    tokenizer_wrapper_class=WrapperClass,\n",
        "    max_seq_length=256, decoder_max_length=3, \n",
        "    batch_size=1,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "AkrXsdLgKwh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Train and inference\n",
        "Done! We can conduct training and inference the same as other processes in Pytorch."
      ],
      "metadata": {
        "id": "rCQSvF5HK0Z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making zero-shot inference using pretrained MLM with prompt\n",
        "import torch\n",
        "promptModel.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "        logits = promptModel(batch)\n",
        "        print(logits)\n",
        "        preds = torch.argmax(logits, dim = -1)\n",
        "        print(classes[preds])\n",
        "# predictions would be 1, 0 for classes 'positive', 'negative'\n"
      ],
      "metadata": {
        "id": "YT4WwqpYK1Z0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}